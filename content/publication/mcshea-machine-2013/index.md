---
# Documentation: https://wowchemy.com/docs/managing-content/

title: Machine Wanting
subtitle: ''
summary: ''
authors:
- Daniel W. McShea
tags:
- Artificial intelligence
- Emotion
- Goal-directedness
- Purpose
- Robot
- Teleology
categories: []
date: '2013-12-01'
lastmod: 2023-09-27T17:46:55-03:00
featured: false
draft: false

# Featured image
# To use, add an image named `featured.jpg/png` to your page's folder.
# Focal points: Smart, Center, TopLeft, Top, TopRight, Left, Right, BottomLeft, Bottom, BottomRight.
image:
  caption: ''
  focal_point: ''
  preview_only: false

# Projects (optional).
#   Associate this post with one or more of your projects.
#   Simply enter your project's folder or file name without extension.
#   E.g. `projects = ["internal-project"]` references `content/project/deep-learning/index.md`.
#   Otherwise, set `projects = []`.
projects: []
publishDate: '2023-09-27T20:46:55.652357Z'
publication_types:
- '2'
abstract: Wants, preferences, and cares are physical things or events, not ideas or
  propositions, and therefore no chain of pure logic can conclude with a want, preference,
  or care. It follows that no pure-logic machine will ever want, prefer, or care.
  And its behavior will never be driven in the way that deliberate human behavior
  is driven, in other words, it will not be motivated or goal directed. Therefore,
  if we want to simulate human-style interactions with the world, we will need to
  first understand the physical structure of goal-directed systems. I argue that all
  such systems share a common nested structure, consisting of a smaller entity that
  moves within and is driven by a larger field that contains it. In such systems,
  the smaller contained entity is directed by the field, but also moves to some degree
  independently of it, allowing the entity to deviate and return, to show the plasticity
  and persistence that is characteristic of goal direction. If all this is right,
  then human want-driven behavior probably involves a behavior-generating mechanism
  that is contained within a neural field of some kind. In principle, for goal directedness
  generally, the containment can be virtual, raising the possibility that want-driven
  behavior could be simulated in standard computational systems. But there are also
  reasons to believe that goal-direction works better when containment is also physical,
  suggesting that a new kind of hardware may be necessary.
publication: ''
doi: 10/gnc646
---
